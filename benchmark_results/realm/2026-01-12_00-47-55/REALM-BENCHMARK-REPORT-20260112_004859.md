# REALM-Bench Benchmark Results

## Summary

| Metric | Value |
|--------|-------|
| **Status** | GOOD |
| **Success Rate** | 53.8% |
| **Total Tasks** | 13 |
| **Passed** | 7 |
| **Failed** | 6 |
| **Estimated Rank** | #6 |
| **Duration** | 64.3s |

## Planning Metrics

| Metric | Value |
|--------|-------|
| Plan Quality | 89.2% |
| Goal Achievement | 75.6% |
| Efficiency | 84.2% |
| Avg Planning Time | 1235ms |
| Avg Execution Time | 3705ms |
| Total Tokens | 17,750 |

## Key Findings

- Good planning performance: 53.8% success rate
- Strongest category: reactive (100.0%)
- Needs improvement: multi_agent (0.0%)
- High plan quality scores achieved
- Excellent execution efficiency
- Outperforms 2/7 baseline models
- Estimated leaderboard rank: #6

## Recommendations

- Focus on improving multi_agent task handling
- Improve plan adaptation strategies

## Category Breakdown

| Category | Total | Passed | Success Rate | Plan Quality |
|----------|-------|--------|--------------|--------------|
| sequential | 3 | 2 | 66.7% | 100.0% |
| reactive | 2 | 2 | 100.0% | 90.0% |
| complex | 2 | 1 | 50.0% | 85.0% |
| multi_agent | 2 | 0 | 0.0% | 65.0% |
| tool_use | 2 | 2 | 100.0% | 100.0% |
| reasoning | 2 | 0 | 0.0% | 90.0% |

## Leaderboard Comparison

| Model | Their Score | Our Score | Difference |
|-------|-------------|-----------|------------|
| GPT-4-Turbo | 72.9% | 53.8% | -19.1% |
| GPT-4 | 69.3% | 53.8% | -15.5% |
| Claude-3-Opus | 67.2% | 53.8% | -13.4% |
| Claude-3-Sonnet | 62.7% | 53.8% | -8.9% |
| Gemini-Pro | 58.7% | 53.8% | -4.9% |
| Llama-3-70B | 52.9% | 53.8% | +0.9% |
| Mixtral-8x7B | 45.8% | 53.8% | +8.0% |

## Configuration

- Model: gpt-4o-mini
- Execution Model: dag
- Max Steps: 15
- Adaptation Enabled: True

---
*Generated by ElizaOS REALM-Bench*
*Timestamp: 2026-01-12T00:48:59.352131*

## Reference

- Paper: https://arxiv.org/abs/2412.13102
- GitHub: https://github.com/genglongling/REALM-Bench
