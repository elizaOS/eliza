# MINT Benchmark Results

## ElizaOS Python Runtime Evaluation

**Benchmark**: MINT (Multi-turn Interaction with Tools and Language Feedback)
**Date**: 2026-01-12T00:45:00.744577
**Duration**: 556.9 seconds
**Total Tasks**: 16

---

## Executive Summary

üåü **Status**: Excellent

**Best Configuration**: baseline
**Best Success Rate**: 75.0%

### Key Findings

- Excellent performance with 75.0% success rate
- Multi-turn interaction provides 12.5% improvement

## Configuration Comparison

| Configuration | Success Rate | Passed | Avg Turns | Avg Latency |
|--------------|--------------|--------|-----------|-------------|
| Baseline (no tools/feedback) | 75.0% | 12/16 | 1.0 | 7209ms |
| Full (tools + feedback) | 68.8% | 11/16 | 1.2 | 27597ms |

### Improvement Analysis

| Metric | Value |
|--------|-------|
| Tool Improvement | +0.0% |
| Feedback Improvement | +0.0% |
| Combined Improvement | -6.2% |
| Synergy Effect | +0.0% |

## Category Breakdown

| Category | Success Rate | Passed | Avg Turns |
|----------|--------------|--------|-----------|
| ‚ö†Ô∏è Reasoning | 50.0% | 2/4 | 3.0 |
| ‚úÖ Coding | 75.0% | 3/4 | 2.5 |
| ‚úÖ Decision Making | 75.0% | 3/4 | 2.0 |
| ‚úÖ Information Seeking | 75.0% | 3/4 | 2.0 |

## Leaderboard Comparison

**ElizaOS Overall Score**: 68.8%

| Model | Published Score | vs ElizaOS |
|-------|----------------|------------|
| gpt-4-0613 | 66.0% | +2.7% |
| gpt-3.5-turbo | 40.0% | +28.7% |
| claude-2 | 61.0% | +7.8% |
| llama-2-70b | 32.0% | +36.8% |

*Note: Leaderboard scores are from the original MINT paper (ICLR 2024).*

## Detailed Metrics

### Performance Metrics

| Metric | Value |
|--------|-------|
| Total Tasks | 16 |
| Passed Tasks | 11 |
| Failed Tasks | 5 |
| Overall Success Rate | 68.8% |
| Average Latency | 27597ms |
| Total Duration | 441.6s |
| Average Tokens/Task | 0 |

### Turn Analysis

| Metric | Value |
|--------|-------|
| Avg Turns to Success | 1.18 |
| Avg Turns to Failure | 5.00 |
| Turn Efficiency | 0.289 |
| Multi-turn Gain | +12.5% |

## Recommendations

1. Continue testing with larger and more diverse datasets
2. Compare with additional model configurations

---

## Methodology

This benchmark follows the MINT evaluation protocol from the ICLR 2024 paper:
"MINT: Evaluating LLMs in Multi-turn Interaction with Tools and Language Feedback"

**Categories Evaluated**:
- Reasoning: Mathematical and logical problems
- Coding: Programming challenges
- Decision Making: Sequential decision tasks
- Information Seeking: Knowledge retrieval tasks

**Configuration**:
- Max turns per task: 5
- Tool execution: Local
- Ablation study: Disabled

---

*Generated by ElizaOS MINT Benchmark Runner*
*Report generated: 2026-01-12T00:45:00.744577*