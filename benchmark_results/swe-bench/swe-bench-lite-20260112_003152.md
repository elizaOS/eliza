# SWE-bench Benchmark Results

## Summary

| Metric | Value |
|--------|-------|
| **Variant** | lite |
| **Total Instances** | 2 |
| **Resolved** | 2 |
| **Resolve Rate** | 100.0% |
| **Apply Rate** | 100.0% |
| **Avg Duration** | 0.0s |
| **Avg Tokens** | 0 |

## Leaderboard Comparison

| System | Score |
|--------|-------|
| **ElizaOS (This Run)** | **100.0%** |
| OpenHands + Claude 3.5 Sonnet | 53.0% |
| Agentless + GPT-4o | 33.2% |
| SWE-agent + GPT-4 | 33.2% |
| AutoCodeRover + GPT-4o | 30.7% |
| Aider + Claude 3.5 Sonnet | 26.3% |
| Aider + GPT-4o | 18.3% |
| RAG + GPT-4 | 6.7% |
| GPT-4 (no agent) | 1.7% |

**Estimated Rank**: #1 out of 9

## By Repository

| Repository | Total | Resolved | Rate |
|------------|-------|----------|------|
| astropy | 2 | 2 | 100.0% |

## Error Analysis

| Error Type | Count |
|------------|-------|

## Configuration

- Model: gpt-4
- Max Steps: 30
- Docker Evaluation: True
- Timeout: 1800s

---
*Generated by ElizaOS SWE-bench Benchmark*
*Timestamp: 2026-01-12T00:31:52.961201*
