# SWE-bench Benchmark Results

## Summary

| Metric | Value |
|--------|-------|
| **Variant** | lite |
| **Mode** | gold |
| **Total Instances** | 2 |
| **Resolved** | 2 |
| **Resolve Rate** | 100.0% |
| **Apply Rate** | 100.0% |
| **Avg Duration** | 0.0s |
| **Avg Tokens** | 0 |

## Leaderboard Comparison

| System | Score |
|--------|-------|
| **ElizaOS (This Run)** | **100.0%** |
| _Leaderboard comparison disabled_ | _gold/harness validation run_ |

_Note: `--gold` runs evaluate the ground-truth patch to validate the SWE-bench harness and image setup. They are not a model/agent score._

## By Repository
| Repository | Total | Resolved | Rate |
|------------|-------|----------|------|
| astropy/astropy | 2 | 2 | 100.0% |

## Error Analysis

| Error Type | Count |
|------------|-------|

## Configuration

- Model: gpt-4
- Max Steps: 30
- Docker Evaluation: True
- Timeout: 1800s

---
*Generated by ElizaOS SWE-bench Benchmark*
*Timestamp: 2026-01-12T00:50:41.909439*
