#!/usr/bin/env node

/**
 * AiNex Robot Visual Assistant Demo
 * Demonstrates the integration of vision and robot control capabilities
 */

// Simulate the createUniqueUuid function since we can't import from @elizaos/core in a standalone demo
function createUniqueUuid(): string {
  return 'xxxxxxxx-xxxx-4xxx-yxxx-xxxxxxxxxxxx'.replace(/[xy]/g, function(c) {
    const r = Math.random() * 16 | 0;
    const v = c === 'x' ? r : (r & 0x3 | 0x8);
    return v.toString(16);
  });
}

// Simulate a mock runtime for demonstration
class MockRuntime {
  agentId = createUniqueUuid();
  character = {
    name: 'AiNex Robot Assistant',
    bio: ['I am an AI-powered humanoid robot with 24 degrees of freedom and visual perception capabilities'],
    system: 'You are controlling an AiNex humanoid robot. You can see through my camera, control my joints, and learn new motions through teaching.'
  };
  
  logger = {
    info: (msg: string, data?: any) => console.log(`[INFO] ${msg}`, data || ''),
    error: (msg: string, data?: any) => console.error(`[ERROR] ${msg}`, data || ''),
    warn: (msg: string, data?: any) => console.warn(`[WARN] ${msg}`, data || ''),
    debug: (msg: string, data?: any) => console.log(`[DEBUG] ${msg}`, data || '')
  };

  getSetting(key: string): string | undefined {
    const settings: Record<string, string> = {
      USE_SIMULATION: 'true',
      ROBOT_SERIAL_PORT: '/dev/ttyUSB0',
      ROBOT_BAUD_RATE: '115200',
      ROS_WEBSOCKET_URL: 'ws://localhost:9090'
    };
    return settings[key];
  }

  services = new Map();
  
  getService(name: string) {
    return this.services.get(name);
  }
}

// Simulate robot state with proper typing
type RobotMode = 'IDLE' | 'MANUAL' | 'AUTONOMOUS' | 'TEACHING' | 'EMERGENCY_STOP';

const robotState = {
  mode: 'IDLE' as RobotMode,
  status: 'OK' as const,
  emergencyStop: false,
  joints: {
    head_yaw: { position: 0, velocity: 0, effort: 0 },
    head_pitch: { position: 0, velocity: 0, effort: 0 },
    left_shoulder_pitch: { position: 0, velocity: 0, effort: 0 },
    left_shoulder_roll: { position: 0, velocity: 0, effort: 0 },
    left_elbow_pitch: { position: 0, velocity: 0, effort: 0 },
    right_shoulder_pitch: { position: 0, velocity: 0, effort: 0 },
    right_shoulder_roll: { position: 0, velocity: 0, effort: 0 },
    right_elbow_pitch: { position: 0, velocity: 0, effort: 0 },
    waist_yaw: { position: 0, velocity: 0, effort: 0 },
    left_hip_pitch: { position: 0, velocity: 0, effort: 0 },
    left_hip_roll: { position: 0, velocity: 0, effort: 0 },
    left_knee_pitch: { position: 0, velocity: 0, effort: 0 },
    left_ankle_pitch: { position: 0, velocity: 0, effort: 0 },
    left_ankle_roll: { position: 0, velocity: 0, effort: 0 },
    right_hip_pitch: { position: 0, velocity: 0, effort: 0 },
    right_hip_roll: { position: 0, velocity: 0, effort: 0 },
    right_knee_pitch: { position: 0, velocity: 0, effort: 0 },
    right_ankle_pitch: { position: 0, velocity: 0, effort: 0 },
    right_ankle_roll: { position: 0, velocity: 0, effort: 0 }
  },
  battery: 85,
  temperature: 32,
  imu: {
    orientation: { x: 0, y: 0, z: 0, w: 1 },
    angularVelocity: { x: 0, y: 0, z: 0 },
    linearAcceleration: { x: 0, y: 0, z: 9.81 }
  }
};

// Simulate vision capabilities
const visionCapabilities = {
  currentScene: {
    objects: ['desk', 'laptop', 'coffee cup', 'person'],
    people: [{ name: 'Unknown', position: 'in front', distance: '2 meters' }],
    text: ['ElizaOS', 'Robot Control'],
    description: 'I see a workspace with a desk, laptop, and coffee cup. There is a person standing in front of me.'
  }
};

// Demonstrate scenarios
async function runScenario(title: string, interactions: Array<{user: string, action: () => void}>) {
  console.log(`\n${'‚ïê'.repeat(60)}`);
  console.log(`üìã ${title}`);
  console.log(`${'‚ïê'.repeat(60)}\n`);
  
  for (const interaction of interactions) {
    console.log(`üë§ User: "${interaction.user}"`);
    await new Promise(resolve => setTimeout(resolve, 500));
    interaction.action();
    console.log();
    await new Promise(resolve => setTimeout(resolve, 1000));
  }
}

// Main demo function
async function main() {
  console.log('‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó');
  console.log('‚ïë          AiNex Robot Visual Assistant Demo                   ‚ïë');
  console.log('‚ïë     Integrating Vision, Control, and Learning                ‚ïë');
  console.log('‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n');

  const runtime = new MockRuntime();

  console.log('ü§ñ Robot Assistant: Hello! I am your AiNex humanoid robot assistant.');
  console.log('   I have 24 degrees of freedom and advanced visual perception.\n');

  // Scenario 1: Visual Perception
  await runScenario('Scenario 1: Visual Perception & Understanding', [
    {
      user: 'What do you see?',
      action: () => {
        console.log('ü§ñ Robot: Looking around...');
        console.log(`   üì∑ I see ${visionCapabilities.currentScene.description}`);
        console.log('   Objects detected:', visionCapabilities.currentScene.objects.join(', '));
      }
    },
    {
      user: 'Can you read any text?',
      action: () => {
        console.log('ü§ñ Robot: Analyzing text in view...');
        console.log(`   üìù I can see text that says: "${visionCapabilities.currentScene.text.join('", "')}"`);
      }
    },
    {
      user: 'Look at the person and wave',
      action: () => {
        console.log('ü§ñ Robot: Tracking person and preparing to wave...');
        console.log('   üëÅÔ∏è Person detected: 2 meters away, directly in front');
        console.log('   üéØ Moving head_yaw to face person (0.0 rad)');
        console.log('   üëã Executing wave motion:');
        console.log('      - right_shoulder_pitch ‚Üí -1.2 rad');
        console.log('      - right_elbow_pitch ‚Üí -0.5 rad');
        console.log('      - Moving hand side to side...');
        console.log('   ‚úÖ Wave complete!');
      }
    }
  ]);

  // Scenario 2: Robot Control
  await runScenario('Scenario 2: Direct Robot Control', [
    {
      user: 'Move your head to look left',
      action: () => {
        console.log('ü§ñ Robot: Moving head to look left...');
        console.log('   üéØ Setting head_yaw to 0.785 rad (45¬∞)');
        robotState.joints.head_yaw.position = 0.785;
        console.log('   ‚úÖ Head moved to left position');
      }
    },
    {
      user: 'Stand in a ready position',
      action: () => {
        console.log('ü§ñ Robot: Moving to ready stance...');
        console.log('   ü¶ø Adjusting leg positions:');
        console.log('      - hip_pitch: -0.1 rad');
        console.log('      - knee_pitch: 0.2 rad');
        console.log('      - ankle_pitch: -0.1 rad');
        console.log('   ü¶æ Adjusting arm positions:');
        console.log('      - shoulder_pitch: -0.2 rad');
        console.log('      - elbow_pitch: -0.5 rad');
        console.log('   ‚úÖ Ready position achieved');
      }
    },
    {
      user: 'Show me your battery level',
      action: () => {
        console.log('ü§ñ Robot: Checking system status...');
        console.log(`   üîã Battery: ${robotState.battery}%`);
        console.log(`   üå°Ô∏è Temperature: ${robotState.temperature}¬∞C`);
        console.log(`   üìä IMU Status: Stable (upright orientation)`);
        console.log(`   ‚öôÔ∏è All servos: Operational`);
      }
    }
  ]);

  // Scenario 3: Teaching Mode
  await runScenario('Scenario 3: Teaching by Demonstration', [
    {
      user: 'Enter teaching mode',
      action: () => {
        robotState.mode = 'TEACHING';
        console.log('ü§ñ Robot: Teaching mode activated!');
        console.log('   üéì Servos are now compliant');
        console.log('   üëê You can now move my joints manually');
        console.log('   üí° Say "record pose" when you want to save a position');
      }
    },
    {
      user: 'Record this pose as greeting',
      action: () => {
        console.log('ü§ñ Robot: Recording current pose...');
        console.log('   üì∏ Captured joint positions:');
        console.log('      - right_shoulder_pitch: -0.8 rad');
        console.log('      - right_elbow_pitch: -1.2 rad');
        console.log('      - left_shoulder_pitch: -0.8 rad');
        console.log('   üíæ Pose saved as "greeting"');
      }
    },
    {
      user: 'Save this motion sequence as handshake',
      action: () => {
        console.log('ü§ñ Robot: Recording motion sequence...');
        console.log('   üé¨ Recording keyframes:');
        console.log('      Frame 1: Arm extended forward');
        console.log('      Frame 2: Hand open position');
        console.log('      Frame 3: Grip position');
        console.log('      Frame 4: Shake motion (3 cycles)');
        console.log('   üíæ Motion "handshake" saved with 4 keyframes');
        console.log('   ‚è±Ô∏è Total duration: 3.5 seconds');
      }
    }
  ]);

  // Scenario 4: Autonomous Behavior
  await runScenario('Scenario 4: Autonomous Visual-Motor Coordination', [
    {
      user: 'Track the coffee cup and point at it',
      action: () => {
        console.log('ü§ñ Robot: Searching for coffee cup...');
        console.log('   üëÅÔ∏è Object detected: Coffee cup at 30¬∞ right, 1.5m away');
        console.log('   üéØ Tracking object:');
        console.log('      - head_yaw ‚Üí -0.524 rad (tracking)');
        console.log('      - head_pitch ‚Üí -0.2 rad (looking down)');
        console.log('   üëâ Pointing gesture:');
        console.log('      - right_shoulder_pitch ‚Üí -0.5 rad');
        console.log('      - right_shoulder_roll ‚Üí -0.3 rad');
        console.log('      - right_elbow_pitch ‚Üí 0.0 rad (extended)');
        console.log('   ‚úÖ Pointing at coffee cup');
      }
    },
    {
      user: 'If you see a person, wave. Otherwise, nod.',
      action: () => {
        console.log('ü§ñ Robot: Analyzing scene...');
        console.log('   üëÅÔ∏è Person detected in view!');
        console.log('   ü§î Decision: Execute wave gesture');
        console.log('   üëã Performing wave motion...');
        console.log('   ‚úÖ Social gesture completed');
      }
    }
  ]);

  // Scenario 5: Safety Features
  await runScenario('Scenario 5: Safety and Emergency Response', [
    {
      user: 'What are your safety limits?',
      action: () => {
        console.log('ü§ñ Robot: My safety features include:');
        console.log('   ‚ö†Ô∏è Joint limits: Each joint has min/max position limits');
        console.log('   üöÑ Velocity limits: Max 2.0 rad/s for safety');
        console.log('   üéØ Acceleration limits: Max 5.0 rad/s¬≤ for smooth motion');
        console.log('   üõ°Ô∏è Collision detection: IMU-based fall detection');
        console.log('   üõë Emergency stop: Instant halt on command');
      }
    },
    {
      user: 'Emergency stop!',
      action: () => {
        robotState.mode = 'EMERGENCY_STOP';
        robotState.emergencyStop = true;
        console.log('ü§ñ Robot: ‚ö†Ô∏è EMERGENCY STOP ACTIVATED!');
        console.log('   üõë All motors halted immediately');
        console.log('   üîí Motion commands disabled');
        console.log('   üì¢ Waiting for reset command...');
      }
    },
    {
      user: 'Reset emergency stop',
      action: () => {
        robotState.mode = 'IDLE';
        robotState.emergencyStop = false;
        console.log('ü§ñ Robot: Emergency stop cleared');
        console.log('   ‚úÖ Systems back online');
        console.log('   üîÑ Running self-diagnostic...');
        console.log('   ‚úÖ All systems operational');
      }
    }
  ]);

  // Summary
  console.log('\n‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó');
  console.log('‚ïë                    Demo Summary                              ‚ïë');
  console.log('‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n');

  console.log('üéØ Demonstrated Capabilities:');
  console.log('   ‚úÖ Visual perception and scene understanding');
  console.log('   ‚úÖ Natural language robot control');
  console.log('   ‚úÖ Teaching by demonstration');
  console.log('   ‚úÖ Visual-motor coordination');
  console.log('   ‚úÖ Safety features and emergency response\n');

  console.log('üîß Technical Features:');
  console.log('   ‚Ä¢ 24 DOF control with safety limits');
  console.log('   ‚Ä¢ Serial communication (115200 baud)');
  console.log('   ‚Ä¢ ROS 2 integration via WebSocket');
  console.log('   ‚Ä¢ IMU-based balance and fall detection');
  console.log('   ‚Ä¢ Motion recording and playback');
  console.log('   ‚Ä¢ Real-time visual processing\n');

  console.log('üìö Available Commands:');
  console.log('   ‚Ä¢ Movement: "Move [joint] to [angle]"');
  console.log('   ‚Ä¢ Vision: "What do you see?", "Look at [object]"');
  console.log('   ‚Ä¢ Teaching: "Enter teaching mode", "Record pose as [name]"');
  console.log('   ‚Ä¢ Motions: "Execute motion [name]", "Wave", "Point at [object]"');
  console.log('   ‚Ä¢ Safety: "Emergency stop", "Show status"\n');

  console.log('üöÄ Next Steps:');
  console.log('   1. Install dependencies: npm install');
  console.log('   2. Run setup: ./scripts/setup-robot.sh');
  console.log('   3. Start simulation or connect hardware');
  console.log('   4. Launch ElizaOS with robot character\n');

  console.log('üí° For more information, see:');
  console.log('   ‚Ä¢ README.md - Getting started guide');
  console.log('   ‚Ä¢ docs/ROBOT_IMPLEMENTATION_COMPLETE.md - Full documentation');
  console.log('   ‚Ä¢ src/tests/e2e/robot-control.ts - Test examples\n');

  console.log('‚ú® Demo complete! The AiNex robot is ready to assist you.');
}

// Run the demo
main().catch(console.error);

// Export to make this a module
export {}; 