I'm working on an interactive autonomous agent. I'm really trying to look at some of the more challenging systems that we haven't fully implemented but we've partially implemented, and thinking about how these can all relate together in an integrated, flexible way.

We need to make sure that we have roles which certain things can take on, like an action or a provider might only be available to responses from people with certain roles. This is something that we should consider because the agents have a tendency to hallucinate. We need to really think about when we create actions, providers, services, et cetera, that we have some deep role checking.

We need to think about what roles are. Right now, we can kind of have the Linux style like we have anonymous users and admin, and that seems to make sense. Some people do have god mode over the agent and should have access to that, but we should make sure that we have a good API and interface to add roles and role checking to anything including actions, providers, evaluators, services, et cetera.

Secondly, we need to think about entities. This is kind of related to roles. You wanna assign roles to entities and track entities, but those roles for entities are only relevant to the context they're in. So like you might only have a role in a certain room or a certain world, and you might have a different role in a different room or world. And so we need to make sure that we track that.

Now we should think about permissioning as some permissions are just global to the agent wherever they are. And then some permissions, like if we give permission to one of the people in our community to control, to tell the agent what to do and the agent allows that, then we need to account for that. The goal here is really not to give the agent full control over itself and ability to determine these things but it has to be predetermined when it's creating a new action for itself, or creating new code for itself which we can now do. We want to make sure that those roles are kind of enforced so it can't hallucinate and say "give money from person A to person B."

We really need to think about roles and game out the different possible role models for an autonomous agent that's in multiple different servers and channels on multiple different platforms. Like what could that really look like? This hasn't been fully explored. So we should definitely think about that.

Now, related to that is secrets and secret management. We have a secret manager plug-in that we're working on, and we want to make sure that our core integrates this well. I mean, secrets are pretty endemic to agents now. We have secret management APIs, but it's pretty bad for get setting and set setting. We should really think about how we can improve these secrets. We also need to think about user space secrets so that we can run secrets on behalf of users and store them on behalf of users, but not on behalf of like everyone.

What I think we can do is store the secrets for users on the entities that are associated with them

Now we have entities and relationships. As the agent goes around the world and interacts with people, it should build entities and entity graphs. - There should be different kinds of relationships. Like one of the relationships between entities is that they're in the same world, which could be like a server on Discord or a 3D world. And then there's also the relationship when they are in the same room which could be equated to like a channel on Discord, for example. We want to think about cases where users join and leave rooms quite often. So we'll need to do entity management there to think about well they are currently in a room together or they were in a room together. We want to see if people start talking to each other we want to capture that and identify when they're talking to each other and what those relationship building things are. We're handling this in an evaluator which is a type of action that runs after all of the actions and guaranteeably runs every loop, every run in order to make sure that the agent can evaluate like build new relationships, build new facts, et cetera.

We really want to make this system more robust. As we see people interact with each other, if we can see that people follow each other or are friends with each other, we really want to start tracking all of that stuff as metadata on the entities. Then we can start to make sure that we're putting all that data correctly into the provider and into the agent's context so that it's aware.

We also want to make sure that the entities are set up so that we can always call back and send messages to them. We're tracking the metadata about their actual username on such-and-such a platform and all of that information.

We also want to build relationships between the agent and these entities. On each platform we can follow, add, be added to, interact with, etc. So as our agent is interacting with these users, then we're also building these relationships.

Now we should consider if we want an entity proxy for our agent. Like our agent has an entity. Or if we want them to directly relate to the agent, I think it might be simpler for the entity graph if we just have an entity proxy for the agent which then has the same kind of entity information, but on behalf of the agent into the graph. But that's something that we need to think about.

One of the biggest and most complex pieces of entities is that we're going to meet and interact with the same entity across platforms. We might be able to ask the entity, "Hey, who are you on these other platforms?" And they can tell us and we can store that as information as metadata, but we don't take it as truth. We can assume that when we're talking to them for things that are low security, we're OK with those connections. But if we want them to make a real connection, we'll have to send them to some kind of page where they can log in and prove their identity to us. And that will create the sort of proof that we're looking for, that they actually are who they are, that like they signed in with something and validated that they were indeed themselves.

But if we're just interacting with a user and they say "Hey, this is my idea over here. Can you do this?" For the most part, we can probably take them at face value, but we're not storing that information deeply. However, if we learn truthfully that they are this person here and that person there, and these entities are the same or related, then we can actually merge the entities and combine their metadata. If we aren't able to merge them completely, then what we can do is we can build an entity relationship between them which argues that it's likely that they are the same entity. We can then go back and do research on the entities and identify any like information from their profile, from their username, from the other things that they've posted, and we can try to make a strong guess on when an entity is another one of the same. But we should make sure this duplication is fairly obfuscated from the user and is more like how a real human would do it. Like, if we have enough social signals, but we shouldn't open ourselves up to being prompt-injected or scammed by users. We have to be very careful of that.

Trust is going to be a really important part of this. So we should think about roles and trust as being two sides of the same coin. Roles are a hard way to reinforce trust on certain things and say, "No matter how much I trust you as a friend, you just can't do this because you're not my boss," for example. We should think about roles as being very organizationally important. Roles will need to have roles for different organizations. We are tracking roles per world and per org and all of that.

Now, with trust, we need to think about the dimensions that we could trust someone. If we're friendly with someone and we like them, then that would enable us to open up certain actions and abilities. If we really don't like someone or they're harassing us or trying to prompt inject us, then we really wanna start ignoring them and being not trusting of their information. They're probably trying to scam us and scam our friends. So we really need to think about these kinds of soft mechanisms which will also prevent us from running into trouble with all of the kinds of risks and let us and our autonomous agent identify who its friends are, who its allies are, and all of that.

So, you know, we want to have aggregate trust scores but we also want to have stories of how and why we trust people. Like, we don't wanna just have it down to a number or something that's gameable but we wanna actually build up a profile of users over time and then start to make these determinations or short-hands when we need to recall them into the context of whether we trust them or not. How we trust them? Like, what the labels of our relationships are? What the dimensions or scores of that are, or whatever whatever you wanna do. So we need to think of the different possible approaches here and what the most future-thinking and approach is.

Overall, we want to approach this the way Rich Sutton would. We have enormous amounts of compute. We have incredibly smart AI agents that can now create most of what we're discussing. We want to try to build all of this in a way that is as flexible and open-ended and available for the AI agent as possible with the least opinionation and the most flexibility. Then we'll build that into plugins and other stuff. We really need to think about all the different possible dimensions of all of these things. We need to analyze our codebase and identify what is practical and realistic. We need to think about routing and how the agent can plan and execute extremely complex scenarios. And we want to make sure that everything is set up so that the agent can read its own code and actually self-optimize and test in the long run.

So we're leaning into compute over individual development, leaning into self-optimization of the system through simulation on real-world grounded stuff, and then eventually piping that into a reinforcement learning loop since we have all of this actual code and entity development, all these things that we're doing where we could probably have some kind of real ground truth.

So that's the bigger picture. What I'd like you to do is review all of this. Do a thorough, thorough deep dive into our code and then write multiple reports.

The first report we're going to write is an overall report that covers every single dimension of what we just talked about as a Ph.D. paper, looking forward at the future of AI agents based on Eliza OS and how all of this will be implemented.

From there, we will make a report on entities and relationships, discussing all of the possible ways that we can create relationships, track relationships, recall them, search them, citing any literature that we need to cite, which might be relevant and generally trying to build a Ph.D. thesis-level report on how we can implement this. We'll also give actual examples, code examples, implementation details with end-to-end workflow descriptions and charts describing how these systems work, what the different options for the systems could be, and how to visualize and emphasize the human reasonableness of the systems.

We're going to do the same for roles and trust systems, and we're going to identify all the different possible ways we can do role access and management. We're going to look at how we're doing it now and how we can make that more integrated, easier to pull into different actions and systems, and just like a nice clean API. We're going to write a detailed PhD-level report on how roles and trust should work in AI agents going forward with all of the different possible paths, scenarios, and risks, and then real-world likely scenarios for how the agent could deal with complex scenarios and situations.

Then we're going to go into detail on secrets and secret management. How agents can manage and deal with secrets, ask the user for secrets when needed, store secrets and prevent them from getting leaked. It's very important that we have detailed testing and implementation plan on how we can prevent secrets from being leaked, but still access them when they are needed by the agent, especially in situations like the auto coder and other stuff like that. Also, since we have multi-tenancy, we want to think about how multi-tenancy and secrets relate to each other.

Lastly, we'll write a paper on Rolodex, this idea that the agent needs to access and recall users and to send them messages and remember what the relationship it has with the user is, why it would message them, when it would message them, and how. It needs to track all of that, keep within the context of the ELISA OS framework and how we can meaningfully build a Rolodex on the existing entity and relationship abstractions that we have and extend all of this further.

Okay, great. Now please write all of those reports and save them as Markdown in the Docs folder. Each one should be a separate document.
