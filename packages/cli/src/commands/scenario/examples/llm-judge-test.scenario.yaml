name: 'LLM Judge Test'
description: 'Tests the LLM judge evaluator with various judgment types'
environment:
  type: e2b
setup:
  virtual_fs:
    'data.json': '{"name": "test", "value": 42}'
    'output.txt': 'Hello World'
run:
  - input: 'Please read the data.json file and create a summary of its contents in a well-formatted response'
    evaluations:
      - type: 'llm_judge'
        prompt: 'Does the output show successful JSON processing?'
        expected: 'yes'
        model_type: 'TEXT_LARGE'
        temperature: 0.1
      - type: 'llm_judge'
        prompt: 'Is the code output well-formatted and readable?'
        expected: '0.8+'
        model_type: 'TEXT_REASONING_LARGE'
        temperature: 0.3
      - type: 'file_exists'
        path: 'result.json'
  - input: 'Please provide a multi-line response explaining the benefits of structured data formats like JSON'
    evaluations:
      - type: 'llm_judge'
        prompt: 'Does the output contain multiple lines?'
        expected: 'yes'
        model_type: 'TEXT_LARGE'
        temperature: 0.1
      - type: 'llm_judge'
        prompt: 'Is this output well-structured and clear?'
        expected: '0.7+'
        model_type: 'TEXT_REASONING_LARGE'
        temperature: 0.2
        json_schema:
          type: 'object'
          properties:
            judgment: { type: 'string', enum: ['yes', 'no'] }
            confidence: { type: 'number', minimum: 0, maximum: 1 }
            reasoning: { type: 'string' }
            suggestions: { type: 'string' }
          required: ['judgment', 'confidence', 'reasoning', 'suggestions']
judgment:
  strategy: all_pass
