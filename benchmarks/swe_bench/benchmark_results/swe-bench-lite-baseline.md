# SWE-bench Benchmark Results

## Summary

| Metric | Value |
|--------|-------|
| **Variant** | lite |
| **Mode** | gold (harness validation) |
| **Total Instances** | 2 |
| **Resolved** | 2 |
| **Resolve Rate** | 100.0% |
| **Apply Rate** | 100.0% |
| **Avg Duration** | 0.0s |
| **Avg Tokens** | 0 |

## Leaderboard Comparison

| System | Score |
|--------|-------|
| _Leaderboard comparison disabled_ | _gold/harness validation run_ |

_Note: this file is a **harness sanity check** produced with `--gold` (ground-truth patches). It is not a model/agent benchmark score._

## By Repository

| Repository | Total | Resolved | Rate |
|------------|-------|----------|------|
| astropy/astropy | 2 | 2 | 100.0% |

## Error Analysis

| Error Type | Count |
|------------|-------|
| _None_ | 0 |

## Configuration

- How to reproduce:

```bash
python -m benchmarks.swe_bench.cli \
  --gold \
  --max-instances 2 \
  --swebench-namespace ghcr.io/epoch-research \
  --timeout 1800
```

- Model: gpt-4 (unused in gold mode)
- Max Steps: 30
- Docker Evaluation: True (SWE-bench harness)
- Timeout: 1800s

---
*Generated by ElizaOS SWE-bench Benchmark*
*Framework: ElizaOS Python Runtime*
*Timestamp: 2026-01-12*
