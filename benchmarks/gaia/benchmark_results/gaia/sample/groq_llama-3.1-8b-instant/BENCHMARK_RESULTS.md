# GAIA Benchmark Results - ElizaOS Python

**Generated:** 2026-01-12T11:29:51.444743

## Executive Summary

| Metric | Value |
|--------|-------|
| **Overall Accuracy** | 0.0% |
| **Total Questions** | 5 |
| **Correct Answers** | 0 |
| **Human Baseline** | 92% |
| **Best AI (h2oGPTe)** | 65% |

## Results by Level

| Level | Questions | Correct | Accuracy |
|-------|-----------|---------|----------|
| Level 1 | 2 | 0 | 0.0% |
| Level 2 | 2 | 0 | 0.0% |
| Level 3 | 1 | 0 | 0.0% |

## Performance Metrics

- **Average Latency:** 20.0 seconds
- **Average Steps:** 15.0 per question
- **Average Tools Used:** 1.4 per question
- **Total Tokens:** 238,724
- **Average Tokens:** 47745 per question
- **Error Rate:** 0.0%

## Tool Usage

| Tool | Uses | Success Rate |
|------|------|-------------|
| calculator | 5 | 0.0% |
| code_exec | 2 | 0.0% |

## Analysis

### Key Findings
- Low performance: 0.0% accuracy

### Strengths

### Areas for Improvement
- Level 1: Only 0.0% accuracy
- Level 2: Only 0.0% accuracy
- Level 3: Only 0.0% accuracy
- calculator: Low success rate (0.0%)

### Recommendations
- Improve calculator tool usage strategy

## Configuration

- **Dataset Source:** sample
- **Provider:** groq
- **Model:** llama-3.1-8b-instant
- **Temperature:** 0.0
- **Max Tokens:** 4096
- **Split:** validation
- **Duration:** 100 seconds
- **Peak Memory:** 3.9 MB

---
*Generated by ElizaOS GAIA Benchmark Runner*
