# Terminal-Bench Evaluation Report

## Summary

| Metric | Value |
|--------|-------|
| **Total Tasks** | 5 |
| **Passed** | 0 |
| **Failed** | 5 |
| **Accuracy** | 0.0% |
| **Total Commands** | 0 |
| **Avg Commands/Task** | 0.0 |
| **Total Tokens** | 0 |
| **Avg Tokens/Task** | 0 |
| **Evaluation Time** | 0.0s |

## Leaderboard Comparison

| System | Score |
|--------|-------|
| Human Expert | 92.5% |
| Droid (Factory) + GPT-5.2 | 64.9% |
| Ante (Antigma Labs) + Gemini 3 Pro | 64.7% |
| Junie CLI (JetBrains) + Gemini 3 Flash | 64.3% |
| Claude Code + Claude 3.5 Sonnet | 58.2% |
| OpenHands + GPT-4o | 52.8% |
| Aider + Claude 3.5 Sonnet | 47.5% |
| GPT-4 (baseline, no agent) | 28.3% |
| **ElizaOS (This Run)** | 0.0% |

**Rank**: #9 out of 9
**Percentile**: 0.0%

*Nearest above*: GPT-4 (baseline, no agent) (28.3%)

## Results by Category

| Category | Total | Passed | Accuracy | Avg Commands |
|----------|-------|--------|----------|--------------|
| scripting | 2 | 0 | 0.0% | 0.0 |
| file_operations | 2 | 0 | 0.0% | 0.0 |
| code_compilation | 1 | 0 | 0.0% | 0.0 |

## Results by Difficulty

| Difficulty | Total | Passed | Accuracy | Avg Commands |
|------------|-------|--------|----------|--------------|
| easy | 2 | 0 | 0.0% | 0.0 |
| medium | 3 | 0 | 0.0% | 0.0 |

## Error Analysis

| Error Type | Count |
|------------|-------|
| Test failed | 5 |

## Task Results

| Task ID | Status | Commands | Time (ms) | Tokens |
|---------|--------|----------|-----------|--------|
| sample_001 | ❌ | 0 | 0 | 0 |
| sample_002 | ❌ | 0 | 0 | 0 |
| sample_003 | ❌ | 0 | 0 | 0 |
| sample_004 | ❌ | 0 | 0 | 0 |
| sample_005 | ❌ | 0 | 0 | 0 |

## Configuration

- **version**: 2.0
- **model**: gpt-4
- **max_iterations**: 20
- **timestamp**: 2026-01-11T23:42:06.584167

---
*Generated by ElizaOS Terminal-Bench*