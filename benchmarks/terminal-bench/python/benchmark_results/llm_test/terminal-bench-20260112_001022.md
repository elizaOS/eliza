# Terminal-Bench Evaluation Report

## Summary

| Metric | Value |
|--------|-------|
| **Total Tasks** | 3 |
| **Passed** | 2 |
| **Failed** | 1 |
| **Accuracy** | 66.7% |
| **Total Commands** | 5 |
| **Avg Commands/Task** | 1.7 |
| **Total Tokens** | 10,434 |
| **Avg Tokens/Task** | 3478 |
| **Evaluation Time** | 45.7s |

## Leaderboard Comparison

| System | Score |
|--------|-------|
| Human Expert | 92.5% |
| **ElizaOS (This Run)** | 66.7% |
| Droid (Factory) + GPT-5.2 | 64.9% |
| Ante (Antigma Labs) + Gemini 3 Pro | 64.7% |
| Junie CLI (JetBrains) + Gemini 3 Flash | 64.3% |
| Claude Code + Claude 3.5 Sonnet | 58.2% |
| OpenHands + GPT-4o | 52.8% |
| Aider + Claude 3.5 Sonnet | 47.5% |
| GPT-4 (baseline, no agent) | 28.3% |

**Rank**: #2 out of 9
**Percentile**: 77.8%

*Nearest above*: Human Expert (92.5%)
*Nearest below*: GPT-4 (baseline, no agent) (28.3%)

## Results by Category

| Category | Total | Passed | Accuracy | Avg Commands |
|----------|-------|--------|----------|--------------|
| file_operations | 1 | 1 | 100.0% | 2.0 |
| scripting | 2 | 1 | 50.0% | 1.5 |

## Results by Difficulty

| Difficulty | Total | Passed | Accuracy | Avg Commands |
|------------|-------|--------|----------|--------------|
| easy | 2 | 2 | 100.0% | 1.5 |
| medium | 1 | 0 | 0.0% | 2.0 |

## Error Analysis

| Error Type | Count |
|------------|-------|
| Test failed | 1 |

## Task Results

| Task ID | Status | Commands | Time (ms) | Tokens |
|---------|--------|----------|-----------|--------|
| sample_001 | ✅ | 1 | 56 | 1861 |
| sample_002 | ✅ | 2 | 91 | 4640 |
| sample_003 | ❌ | 2 | 89 | 3933 |

## Configuration

- **version**: 2.0
- **model**: gpt-4o-mini
- **max_iterations**: 10
- **timestamp**: 2026-01-12T00:10:22.895480

---
*Generated by ElizaOS Terminal-Bench*