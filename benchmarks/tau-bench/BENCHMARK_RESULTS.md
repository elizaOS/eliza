# ElizaOS Tau-bench Benchmark Results

**Date**: January 12, 2026  
**Benchmark Version**: 0.1.0  
**ElizaOS Version**: 1.0.0

## Summary

These results were generated by running the Tau-bench Python harness in **Real LLM (ElizaOS)** mode using the **OpenAI** model provider plugin.

- **Dataset**: `benchmarks/tau-bench/python/benchmark-data/tau-bench` (18 tasks: 8 retail, 10 airline)
- **Models**: configured via `OPENAI_SMALL_MODEL` / `OPENAI_LARGE_MODEL` (run used `gpt-4o-mini`)
- **Trials per task**: 1
- **Temperature**: 0.0

| Metric | Score |
|--------|-------|
| **Overall Success Rate (Pass^1)** | **100.0% (18/18)** |
| **Tool Accuracy** | 68.1% |
| **Policy Compliance** | 100.0% |
| **Response Quality** | 51.4% |
| **Avg Duration** | 4091ms |

## Domain Performance

- **Retail**: 8/8 passed (100.0%)
- **Airline**: 10/10 passed (100.0%)

## Leaderboard Comparison (Reference)

The harness prints a leaderboard comparison using `elizaos_tau_bench.constants.LEADERBOARD_SCORES` as a **reference baseline**.

Note: the official τ-bench leaderboard is computed on the official τ-bench dataset; scores are **not directly comparable** to this repo’s local dataset.

## Reproducibility

```bash
cd benchmarks/tau-bench/python

# Install deps + dev tools
pip install -e ".[dev]"

# Install the OpenAI model provider plugin
pip install -e ../../../plugins/plugin-openai/python

# Set credentials (in your shell or via a .env loader)
export OPENAI_API_KEY="..."

# Optional: choose models
export OPENAI_SMALL_MODEL="gpt-4o-mini"
export OPENAI_LARGE_MODEL="gpt-4o-mini"

# Run full dataset (real LLM)
python -m elizaos_tau_bench --all --real-llm --trials 1 --timeout 120000 --output ./benchmark_results/real_openai_full_1trial_v2
```

## Raw Results

- **Output directory**: `benchmarks/tau-bench/python/benchmark_results/real_openai_full_1trial_v2/`
- **Files**:
  - `tau-bench-results.json`
  - `tau-bench-summary.md`
  - `tau-bench-detailed.json`
