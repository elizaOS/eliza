# ============================================================================
# Autonomous Agent Configuration - Python
# Copy this file to .env and customize values
# ============================================================================

# -----------------------------------------------------------------------------
# LOCAL AI CONFIGURATION
# -----------------------------------------------------------------------------
# Directory where GGUF models are stored
# Default: ~/.eliza/models
MODELS_DIR=~/.eliza/models

# Model filename for inference
# Download from: https://huggingface.co/Qwen/Qwen3-4B-GGUF
LOCAL_SMALL_MODEL=Qwen3-4B-Q4_K_M.gguf

# Model context window size
CONTEXT_SIZE=8192

# Number of layers to offload to GPU (0 = CPU only)
GPU_LAYERS=0

# Generation temperature (0.0-2.0)
TEMPERATURE=0.7

# Maximum tokens per generation
MAX_TOKENS=512

# -----------------------------------------------------------------------------
# AUTONOMY CONFIGURATION
# -----------------------------------------------------------------------------
# Sandbox directory where the agent operates
# Default: ../sandbox (relative to script)
SANDBOX_DIR=

# Interval between loop iterations in milliseconds
LOOP_INTERVAL_MS=3000

# Maximum number of iterations before stopping
MAX_ITERATIONS=1000

# Maximum consecutive failures before stopping
MAX_CONSECUTIVE_FAILURES=5

# Number of recent memory records to include in context
MEMORY_CONTEXT_SIZE=10

# Set to "false" to stop the agent
AUTONOMY_ENABLED=true

# Agent UUID (auto-generated if not set)
AGENT_ID=

# Conversation/session UUID (auto-generated if not set)
CONVERSATION_ID=

# -----------------------------------------------------------------------------
# SHELL CONFIGURATION
# -----------------------------------------------------------------------------
# Command timeout in milliseconds
SHELL_TIMEOUT=30000
