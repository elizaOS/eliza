https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching#why-am-i-seeing-typeerror-cannot-read-properties-of-undefined-reading-messages
Anthropic home pagelight logo
English

Search...
Ctrl K
Research
News
Go to claude.ai

Welcome
User Guides
API Reference
Prompt Library
Release Notes
Developer Newsletter
Developer Console
Developer Discord
Support
Get started
Overview
Initial setup
Intro to Claude
Learn about Claude
Use cases
Models
Security and compliance
Build with Claude
Define success criteria
Develop test cases
Prompt engineering
Text generation
Embeddings
Google Sheets add-on
Vision
Tool use (function calling)
Model Context Protocol (MCP)
Computer use (beta)
Prompt caching
Batch processing
PDF support
Citations
Token counting
Multilingual support
Test and evaluate
Strengthen guardrails
Using the Evaluation Tool
Administration
Admin API
Resources
Glossary
Model deprecations
System status
Claude 3 model card
Anthropic Cookbook
Anthropic Courses
Legal center
Anthropic Privacy Policy
Build with Claude
Prompt caching
Prompt caching is a powerful feature that optimizes your API usage by allowing resuming from specific prefixes in your prompts. This approach significantly reduces processing time and costs for repetitive tasks or prompts with consistent elements.

Here’s an example of how to implement prompt caching with the Messages API using a cache_control block:


Shell

Python

curl https://api.anthropic.com/v1/messages \
  -H "content-type: application/json" \
  -H "x-api-key: $ANTHROPIC_API_KEY" \
  -H "anthropic-version: 2023-06-01" \
  -d '{
    "model": "claude-3-5-sonnet-20241022",
    "max_tokens": 1024,
    "system": [
      {
        "type": "text",
        "text": "You are an AI assistant tasked with analyzing literary works. Your goal is to provide insightful commentary on themes, characters, and writing style.\n"
      },
      {
        "type": "text",
        "text": "<the entire contents of Pride and Prejudice>",
        "cache_control": {"type": "ephemeral"}
      }
    ],
    "messages": [
      {
        "role": "user",
        "content": "Analyze the major themes in Pride and Prejudice."
      }
    ]
  }'
In this example, the entire text of “Pride and Prejudice” is cached using the cache_control parameter. This enables reuse of this large text across multiple API calls without reprocessing it each time. Changing only the user message allows you to ask various questions about the book while utilizing the cached content, leading to faster responses and improved efficiency.

​
How prompt caching works
When you send a request with prompt caching enabled:

The system checks if a prompt prefix, up to a specified cache breakpoint, is already cached from a recent query.
If found, it uses the cached version, reducing processing time and costs.
Otherwise, it processes the full prompt and caches the prefix once the response begins.
This is especially useful for:

Prompts with many examples
Large amounts of context or background information
Repetitive tasks with consistent instructions
Long multi-turn conversations
The cache has a 5-minute lifetime, refreshed each time the cached content is used.

Prompt caching caches the full prefix

Prompt caching references the entire prompt - tools, system, and messages (in that order) up to and including the block designated with cache_control.

​
Pricing
Prompt caching introduces a new pricing structure. The table below shows the price per token for each supported model:

Model	Base Input Tokens	Cache Writes	Cache Hits	Output Tokens
Claude 3.5 Sonnet	$3 / MTok	$3.75 / MTok	$0.30 / MTok	$15 / MTok
Claude 3.5 Haiku	$0.80 / MTok	$1 / MTok	$0.08 / MTok	$4 / MTok
Claude 3 Haiku	$0.25 / MTok	$0.30 / MTok	$0.03 / MTok	$1.25 / MTok
Claude 3 Opus	$15 / MTok	$18.75 / MTok	$1.50 / MTok	$75 / MTok
Note:

Cache write tokens are 25% more expensive than base input tokens
Cache read tokens are 90% cheaper than base input tokens
Regular input and output tokens are priced at standard rates
​
How to implement prompt caching
​
Supported models
Prompt caching is currently supported on:

Claude 3.5 Sonnet
Claude 3.5 Haiku
Claude 3 Haiku
Claude 3 Opus
​
Structuring your prompt
Place static content (tool definitions, system instructions, context, examples) at the beginning of your prompt. Mark the end of the reusable content for caching using the cache_control parameter.

Cache prefixes are created in the following order: tools, system, then messages.

Using the cache_control parameter, you can define up to 4 cache breakpoints, allowing you to cache different reusable sections separately. For each breakpoint, the system will automatically check for cache hits at previous positions and use the longest matching prefix if one is found.

​
Cache Limitations
The minimum cacheable prompt length is:

1024 tokens for Claude 3.5 Sonnet and Claude 3 Opus
2048 tokens for Claude 3.5 Haiku and Claude 3 Haiku
Shorter prompts cannot be cached, even if marked with cache_control. Any requests to cache fewer than this number of tokens will be processed without caching. To see if a prompt was cached, see the response usage fields.

For concurrent requests, note that a cache entry only becomes available after the first response begins. If you need cache hits for parallel requests, wait for the first response before sending subsequent requests.

The cache has a 5 minute time to live (TTL). Currently, “ephemeral” is the only supported cache type, which corresponds to this 5-minute lifetime.

​
What can be cached
Every block in the request can be designated for caching with cache_control. This includes:

Tools: Tool definitions in the tools array
System messages: Content blocks in the system array
Messages: Content blocks in the messages.content array, for both user and assistant turns
Images: Content blocks in the messages.content array, in user turns
Tool use and tool results: Content blocks in the messages.content array, in both user and assistant turns
Each of these elements can be marked with cache_control to enable caching for that portion of the request.

​
Tracking cache performance
Monitor cache performance using these API response fields, within usage in the response (or message_start event if streaming):

cache_creation_input_tokens: Number of tokens written to the cache when creating a new entry.
cache_read_input_tokens: Number of tokens retrieved from the cache for this request.
input_tokens: Number of input tokens which were not read from or used to create a cache.
​
Best practices for effective caching
To optimize prompt caching performance:

Cache stable, reusable content like system instructions, background information, large contexts, or frequent tool definitions.
Place cached content at the prompt’s beginning for best performance.
Use cache breakpoints strategically to separate different cacheable prefix sections.
Regularly analyze cache hit rates and adjust your strategy as needed.
​
Optimizing for different use cases
Tailor your prompt caching strategy to your scenario:

Conversational agents: Reduce cost and latency for extended conversations, especially those with long instructions or uploaded documents.
Coding assistants: Improve autocomplete and codebase Q&A by keeping relevant sections or a summarized version of the codebase in the prompt.
Large document processing: Incorporate complete long-form material including images in your prompt without increasing response latency.
Detailed instruction sets: Share extensive lists of instructions, procedures, and examples to fine-tune Claude’s responses. Developers often include an example or two in the prompt, but with prompt caching you can get even better performance by including 20+ diverse examples of high quality answers.
Agentic tool use: Enhance performance for scenarios involving multiple tool calls and iterative code changes, where each step typically requires a new API call.
Talk to books, papers, documentation, podcast transcripts, and other longform content: Bring any knowledge base alive by embedding the entire document(s) into the prompt, and letting users ask it questions.
​
Troubleshooting common issues
If experiencing unexpected behavior:

Ensure cached sections are identical and marked with cache_control in the same locations across calls
Check that calls are made within the 5-minute cache lifetime
Verify that tool_choice and image usage remain consistent between calls
Validate that you are caching at least the minimum number of tokens
While the system will attempt to use previously cached content at positions prior to a cache breakpoint, you may use an additional cache_control parameter to guarantee cache lookup on previous portions of the prompt, which may be useful for queries with very long lists of content blocks
Note that changes to tool_choice or the presence/absence of images anywhere in the prompt will invalidate the cache, requiring a new cache entry to be created.

​
Cache Storage and Sharing
Organization Isolation: Caches are isolated between organizations. Different organizations never share caches, even if they use identical prompts..

Exact Matching: Cache hits require 100% identical prompt segments, including all text and images up to and including the block marked with cache control. The same block must be marked with cache_control during cache reads and creation.

Output Token Generation: Prompt caching has no effect on output token generation. The response you receive will be identical to what you would get if prompt caching was not used.

​
Prompt caching examples
To help you get started with prompt caching, we’ve prepared a prompt caching cookbook with detailed examples and best practices.

Below, we’ve included several code snippets that showcase various prompt caching patterns. These examples demonstrate how to implement caching in different scenarios, helping you understand the practical applications of this feature:


Large context caching example


Caching tool definitions


Continuing a multi-turn conversation

​
FAQ

What is the cache lifetime?

The cache has a lifetime (TTL) of about 5 minutes. This lifetime is refreshed each time the cached content is used.


How many cache breakpoints can I use?

You can define up to 4 cache breakpoints (using cache_control parameters) in your prompt.


Is prompt caching available for all models?

No, prompt caching is currently only available for Claude 3.5 Sonnet, Claude 3 Haiku, and Claude 3 Opus.


How do I enable prompt caching?

To enable prompt caching, include at least one cache_control breakpoint in your API request.


Can I use prompt caching with other API features?

Yes, prompt caching can be used alongside other API features like tool use and vision capabilities. However, changing whether there are images in a prompt or modifying tool use settings will break the cache.


How does prompt caching affect pricing?

Prompt caching introduces a new pricing structure where cache writes cost 25% more than base input tokens, while cache hits cost only 10% of the base input token price.


Can I manually clear the cache?

Currently, there’s no way to manually clear the cache. Cached prefixes automatically expire after 5 minutes of inactivity.


How can I track the effectiveness of my caching strategy?

You can monitor cache performance using the cache_creation_input_tokens and cache_read_input_tokens fields in the API response.


What can break the cache?

Changes that can break the cache include modifying any content, changing whether there are any images (anywhere in the prompt), and altering tool_choice.type. Any of these changes will require creating a new cache entry.


How does prompt caching handle privacy and data separation?

Prompt caching is designed with strong privacy and data separation measures:

Cache keys are generated using a cryptographic hash of the prompts up to the cache control point. This means only requests with identical prompts can access a specific cache.

Caches are organization-specific. Users within the same organization can access the same cache if they use identical prompts, but caches are not shared across different organizations, even for identical prompts.

The caching mechanism is designed to maintain the integrity and privacy of each unique conversation or context.

It’s safe to use cache_control anywhere in your prompts. For cost efficiency, it’s better to exclude highly variable parts (e.g., user’s arbitrary input) from caching.

These measures ensure that prompt caching maintains data privacy and security while offering performance benefits.


Can I use prompt caching with the Batches API?

Yes, it is possible to use prompt caching with your Batches API requests. However, because asynchronous batch requests can be processed concurrently and in any order, cache hits are provided on a best-effort basis.


Why am I seeing the error `AttributeError: 'Beta' object has no attribute 'prompt_caching'` in Python?

This error typically appears when you have upgraded your SDK or you are using outdated code examples. Prompt caching is now generally available, so you no longer need the beta prefix. Instead of:


Python

python client.beta.prompt_caching.messages.create(...)
Simply use:


Python

python client.messages.create(...)

Why am I seeing 'TypeError: Cannot read properties of undefined (reading 'messages')'?

This error typically appears when you have upgraded your SDK or you are using outdated code examples. Prompt caching is now generally available, so you no longer need the beta prefix. Instead of:

TypeScript

client.beta.promptCaching.messages.create(...)
Simply use:


client.messages.create(...)
Was this page helpful?


Yes

No
Computer use (beta)
Batch processing
x
linkedin
On this page
How prompt caching works
Pricing
How to implement prompt caching
Supported models
Structuring your prompt
Cache Limitations
What can be cached
Tracking cache performance
Best practices for effective caching
Optimizing for different use cases
Troubleshooting common issues
Cache Storage and Sharing
Prompt caching examples
FAQ
